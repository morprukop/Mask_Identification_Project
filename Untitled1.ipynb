{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8968baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3090a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_da = 'C:/Users/Morga/Downloads/NFLX_Team2_Project3/simple_images/Test'\n",
    "Train_da = 'C:/Users/Morga/Downloads/NFLX_Team2_Project3/simple_images/Train'\n",
    "Validation_da = 'C:/Users/Morga/Downloads/NFLX_Team2_Project3/simple_images/Validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b843f95",
   "metadata": {},
   "source": [
    "## Keras Image augmentation \n",
    "Helps expand the data set in real time by applying transformations to our images in our dataset. \n",
    "\n",
    "Helps make our model robust.\n",
    "\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1924c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n",
      "Found 1229 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image size\n",
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "\n",
    "# data argumentation\n",
    "DataGen = ImageDataGenerator(rotation_range=45,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True,\n",
    "                                       vertical_flip=True,\n",
    "                                       rescale=1/255.0,width_shift_range=0.2,\n",
    "                                     height_shift_range=0.2)\n",
    "\n",
    "Train_data = DataGen.flow_from_directory(Train_da, target_size=(IMG_SIZE), \n",
    "                                         color_mode=\"rgb\",\n",
    "                                         batch_size=200, \n",
    "                                         shuffle=True,\n",
    "                                         class_mode=\"categorical\")\n",
    "\n",
    "Test_data = DataGen.flow_from_directory(Test_da, target_size=(IMG_SIZE), \n",
    "                                         color_mode=\"rgb\",\n",
    "                                         batch_size=64, \n",
    "                                         shuffle=True,\n",
    "                                         class_mode=\"categorical\")\n",
    "\n",
    "Validation_data = DataGen.flow_from_directory(Validation_da, target_size=(IMG_SIZE), \n",
    "                                         color_mode=\"rgb\",\n",
    "                                         batch_size=64, \n",
    "                                         shuffle=True,\n",
    "                                         class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec5a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = Train_data.classes\n",
    "# test_y=Test_data.classes\n",
    "# val_y=Validation_data.classes\n",
    "hj\n",
    "\n",
    "# training_set.class_indices\n",
    "# train_y.shape,test_y.shape,val_y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9b576",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/beginners-guide-on-image-classification-vgg-19-resnet-50-and-inceptionresnetv2-with-tensorflow-4909c6478941\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03e1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# vgg19 = VGG19(weights = 'imagenet', include_top = False, input_shape = (244,244,3))\n",
    "\n",
    "# for layer in vgg19.layers:\n",
    "#     layer.trainable = False \n",
    "\n",
    "# model.add(vgg19)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(16, activation=\"relu\"))\n",
    "# model.add(Dense(16, activation=\"sigmoid\"))\n",
    "# model.summary()\n",
    "\n",
    "# # model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics =[\"accuracy\"])\n",
    "# model.compile(\n",
    "#   loss='sparse_categorical_crossentropy',\n",
    "#   optimizer=\"adam\",\n",
    "#   metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc29ce04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 4, 4, 512)         20024384  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1048704   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,073,346\n",
      "Trainable params: 1,048,962\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# using vgg19 model\n",
    "vgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
    "\n",
    "# make pre trained model into non trainable bcoz its takes much time\n",
    "for layer in vgg19.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add vgg19 model into our sequence model\n",
    "model.add(vgg19)\n",
    "\n",
    "# flatten the model\n",
    "model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(28, activation=\"relu\",  kernel_initializer=\"he_uniform\"))\n",
    "# model.add(Dense(2, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\"))\n",
    "\n",
    "# Adding dense layers\n",
    "model.add(Dense(128, activation=\"relu\", kernel_initializer=\"he_uniform\"))\n",
    "\n",
    "# Adding output layers\n",
    "model.add(Dense(2, activation='sigmoid', kernel_initializer=\"glorot_uniform\"))\n",
    "\n",
    "# Finaly compile the model...\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "50/50 [==============================] - 983s 20s/step - loss: 0.6172 - accuracy: 0.7859 - val_loss: 0.2913 - val_accuracy: 0.8813\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 990s 20s/step - loss: 0.2169 - accuracy: 0.9176 - val_loss: 0.2109 - val_accuracy: 0.9262\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 1035s 21s/step - loss: 0.1765 - accuracy: 0.9355 - val_loss: 0.1903 - val_accuracy: 0.9200\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 861s 17s/step - loss: 0.1593 - accuracy: 0.9409 - val_loss: 0.1482 - val_accuracy: 0.9425\n",
      "Epoch 5/15\n",
      "50/50 [==============================] - 1024s 21s/step - loss: 0.1466 - accuracy: 0.9466 - val_loss: 0.1690 - val_accuracy: 0.9362\n",
      "Epoch 6/15\n",
      "50/50 [==============================] - 1386s 28s/step - loss: 0.1312 - accuracy: 0.9498 - val_loss: 0.1635 - val_accuracy: 0.9425\n",
      "Epoch 7/15\n",
      "50/50 [==============================] - 1165s 23s/step - loss: 0.1256 - accuracy: 0.9531 - val_loss: 0.1636 - val_accuracy: 0.9400\n",
      "Epoch 8/15\n",
      "50/50 [==============================] - 1273s 25s/step - loss: 0.1251 - accuracy: 0.9557 - val_loss: 0.1137 - val_accuracy: 0.9613\n",
      "Epoch 9/15\n",
      "50/50 [==============================] - 1304s 26s/step - loss: 0.1223 - accuracy: 0.9537 - val_loss: 0.1094 - val_accuracy: 0.9575\n",
      "Epoch 10/15\n",
      "50/50 [==============================] - 1327s 27s/step - loss: 0.1103 - accuracy: 0.9583 - val_loss: 0.1271 - val_accuracy: 0.9488\n",
      "Epoch 11/15\n",
      "50/50 [==============================] - 1322s 26s/step - loss: 0.1158 - accuracy: 0.9549 - val_loss: 0.1230 - val_accuracy: 0.9500\n",
      "Epoch 12/15\n",
      "50/50 [==============================] - 1275s 25s/step - loss: 0.1006 - accuracy: 0.9623 - val_loss: 0.1292 - val_accuracy: 0.9525\n",
      "Epoch 13/15\n",
      "50/50 [==============================] - 1151s 23s/step - loss: 0.1092 - accuracy: 0.9583 - val_loss: 0.1219 - val_accuracy: 0.9575\n",
      "Epoch 14/15\n",
      "50/50 [==============================] - 949s 19s/step - loss: 0.1064 - accuracy: 0.9583 - val_loss: 0.1155 - val_accuracy: 0.9575\n",
      "Epoch 15/15\n",
      "37/50 [=====================>........] - ETA: 3:46 - loss: 0.1057 - accuracy: 0.9601"
     ]
    }
   ],
   "source": [
    "Model = model.fit(Train_data, validation_data=(Validation_data), epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca1659",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/feyzazkefe/mask-and-social-distancing-detection-using-vgg19\n",
    "\n",
    "https://www.kaggle.com/sathishpbs/face-detectmodel-using-vgg19\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/07/step-by-step-guide-for-image-classification-on-custom-datasets/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/01/image-classification-using-convolutional-neural-networks-a-step-by-step-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082981e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
